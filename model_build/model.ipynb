{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn \n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('heart.csv')\n",
    "df[\"Sex\"].replace({'M': 0, 'F': 1}, inplace=True)\n",
    "df[\"ChestPainType\"].replace({\"ATA\": 0, \"NAP\": 1, \"ASY\": 2, \"TA\": 3}, inplace=True)\n",
    "df[\"RestingECG\"].replace({\"Normal\": 0, \"ST\": 1, \"LVH\": 2}, inplace=True)\n",
    "df[\"ExerciseAngina\"].replace({'N': 0, 'Y': 1}, inplace=True)\n",
    "df[\"ST_Slope\"].replace({\"Up\": 0, \"Flat\": 1, \"Down\": 2}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['ST_Slope',\n",
    " 'ChestPainType',\n",
    " 'ExerciseAngina',\n",
    " 'Cholesterol',\n",
    " 'MaxHR',\n",
    " 'Oldpeak',\n",
    " 'Sex',\n",
    " 'FastingBS',\n",
    " 'Age',\n",
    " 'RestingBP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, x_test, Y, y_test = train_test_split(df[features], df['HeartDisease'], train_size=0.8, test_size=0.2, random_state=42)\n",
    "X_train, x_valid, Y_train, y_valid = train_test_split(X,Y, test_size=0.25, train_size=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train => 60%, test=> 20%, valid => 20%\n",
    "X_train.shape, Y_train.shape, x_test.shape, x_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class STPDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data \n",
    "    \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = STPDataset(torch.FloatTensor(X_train.values), torch.FloatTensor(Y_train.values))\n",
    "test_data = STPDataset(torch.FloatTensor(x_test.values), torch.FloatTensor(y_test.values))\n",
    "valid_data = STPDataset(torch.FloatTensor(x_valid.values), torch.FloatTensor(y_valid.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_loader = DataLoader(train_data, batch_size=9, shuffle=True)\n",
    "valid_loader = DataLoader(valid_data, batch_size=1)\n",
    "test_loader = DataLoader(test_data, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_loader), len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeartNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, hidden_dim, dropout_prob):\n",
    "        super(HeartNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.activation = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HeartNN = HeartNet(10, 1, 96, 0.08610917049085276)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HeartNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss() #multiple feature use binary cross entropy loss\n",
    "optimizer = Adam(HeartNN.parameters(), lr=0.004773449431112121)\n",
    "EPOCHS = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = iter(train_loader).__next__() \n",
    "sample[0].shape, sample[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample[0], sample[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "\n",
    "early_stopping_patience = 15\n",
    "early_stopping_counter = 0\n",
    "\n",
    "train_acc = []\n",
    "train_loss = []\n",
    "\n",
    "valid_acc = []\n",
    "valid_loss = []\n",
    "\n",
    "total_step = len(train_loader)\n",
    "total_step_val = len(valid_loader)\n",
    "\n",
    "valid_loss_min=np.inf\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    running_loss=0\n",
    "    correct=0\n",
    "    total=0\n",
    "    \n",
    "    #TRAINING\n",
    "\n",
    "    HeartNN.train()\n",
    "\n",
    "    for batch_idx, (X_train_batch, y_train_batch) in enumerate(train_loader):\n",
    "        X_train_batch, y_train_batch = X_train_batch.to(DEVICE), y_train_batch.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = HeartNN(X_train_batch)\n",
    "        y_pred = torch.round(torch.sigmoid(output))\n",
    "        #LOSS\n",
    "        loss = criterion(output, y_train_batch.unsqueeze(1))\n",
    "        writer.add_scalar(\"Loss/train\", loss, epoch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss+=loss.item() #sum loss for every batch\n",
    "        #ACCURACY\n",
    "        correct += torch.sum(y_pred==y_train_batch.unsqueeze(1)).item()\n",
    "        writer.add_scalar(\"Accuracy/train\", correct, epoch)\n",
    "        total += y_train_batch.size(0)\n",
    "    train_acc.append(100 * correct / total) #calculate accuracy among all entries in the batches\n",
    "    train_loss.append(running_loss/total_step)  #get average loss among all batches dividing total loss by the number of batches\n",
    "\n",
    "    # VALIDATION\n",
    "    correct_v = 0\n",
    "    total_v = 0\n",
    "    batch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        HeartNN.eval()\n",
    "        for batch_idx, (X_valid_batch, y_valid_batch) in enumerate(valid_loader):\n",
    "            X_valid_batch,y_valid_batch=X_valid_batch.to(DEVICE),y_valid_batch.to(DEVICE)\n",
    "            #PREDICTION\n",
    "            output = HeartNN(X_valid_batch)\n",
    "            y_pred = torch.round(torch.sigmoid(output))\n",
    "            #LOSS\n",
    "            loss_v = criterion(output, y_valid_batch.unsqueeze(1))\n",
    "            writer.add_scalar(\"Loss/validation\", correct, epoch)\n",
    "            batch_loss+=loss_v.item()\n",
    "            #ACCURACY\n",
    "            correct_v += torch.sum(y_pred==y_valid_batch.unsqueeze(1)).item()\n",
    "            writer.add_scalar(\"Accuracy/validation\", correct, epoch)\n",
    "            total_v += y_valid_batch.size(0)\n",
    "        valid_acc.append(100 * correct_v / total_v) \n",
    "        valid_loss.append(batch_loss/total_step_val)\n",
    "    \n",
    "    \n",
    "    if np.mean(valid_loss) <= valid_loss_min:\n",
    "        torch.save(HeartNN.state_dict(), './state_dict.pt')\n",
    "        print(f'Epoch {epoch + 0:01}: Validation loss decreased ({valid_loss_min:.6f} --> {np.mean(valid_loss):.6f}).  Saving model ...')\n",
    "        valid_loss_min = np.mean(valid_loss)\n",
    "        early_stopping_counter=0 #reset counter if validation loss decreases\n",
    "    else:\n",
    "        print(f'Epoch {epoch + 0:01}: Validation loss did not decrease')\n",
    "        early_stopping_counter+=1\n",
    "\n",
    "    if early_stopping_counter > early_stopping_patience:\n",
    "        print('Early stopped at epoch :', epoch)\n",
    "        break\n",
    "\n",
    "    print(f'\\t Train_Loss: {np.mean(train_loss):.4f} Train_Acc: {(100 * correct / total):.3f} Val_Loss: {np.mean(valid_loss):.4f}  BEST VAL Loss: {valid_loss_min:.4f}  Val_Acc: {(100 * correct_v / total_v):.3f}\\n')\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob_list = []\n",
    "y_pred_list = []\n",
    "\n",
    "\n",
    "# Loading the best model\n",
    "HeartNN.load_state_dict(torch.load('./state_dict.pt'))\n",
    "\n",
    "with torch.no_grad():\n",
    "\tHeartNN.eval()\n",
    "\tfor batch_idx, (X_test_batch, y_test_batch) in enumerate(test_loader):\n",
    "\t\tX_test_batch = X_test_batch.to(DEVICE)\n",
    "\t\t#PREDICTION\n",
    "\t\toutput = HeartNN(X_test_batch)\n",
    "\t\ty_pred_prob = torch.sigmoid(output)\n",
    "\t\ty_pred_prob_list.append(y_pred_prob.cpu().numpy())\n",
    "\t\ty_pred = torch.round(y_pred_prob)\n",
    "\t\ty_pred_list.append(y_pred.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob_list = [a.squeeze().tolist() for a in y_pred_prob_list]\n",
    "y_pred_list = [a.squeeze().tolist() for a in y_pred_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_test, y_pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_test, y_pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test, y_pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.plot(train_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(valid_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(valid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import onnxruntime\n",
    "\n",
    "HeartNN = HeartNet(10, 1, 96, 0.08610917049085276)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HeartNN.load_state_dict(torch.load(\"state_dict.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "dummy_input = torch.randn(input_shape)\n",
    "output_path = \"HeartNet.onnx\"\n",
    "torch.onnx.export(HeartNN, dummy_input, output_path, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import onnxruntime\n",
    "onnx_model = onnx.load(output_path)\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = [[  1.0000,   2.0000,   1.0000, 217.0000, 110.0000,   2.5000,   0.0000,\n",
    "            0.0000,  55.0000, 158.0000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "model_path = 'HeartNet.onnx'\n",
    "session = ort.InferenceSession(model_path)\n",
    "input_name = session.get_inputs()[0].name\n",
    "output_name = session.get_outputs()[0].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = session.run([output_name], {input_name: input})[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(sigmoid(output))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "heart_stroke",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0138b1d543c515e8d2e712519d5c22bf38ed694e5670a9715b1c38393e0a041e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
